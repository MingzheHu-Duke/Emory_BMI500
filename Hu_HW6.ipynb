{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hu_HW6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOevjY5EyFsdirQIRv9aWvR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MingzheHu-Duke/Emory_BMI500/blob/main/Hu_HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mQALUij2HPe"
      },
      "source": [
        "SUBMISSION PROTOCOL:   \n",
        "Put all code in a .py file.   \n",
        "Prepare a report for the answers to the following questions.   \n",
        "Submit both the .py files and the report.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WryVx4Ys5B5G"
      },
      "source": [
        "# import the packages\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWu0d-v_2X56"
      },
      "source": [
        "1. You are provided two text files (text1 and text2)  \n",
        "a) Preprocess the two texts by lowercasing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjvJsRrr2dr9"
      },
      "source": [
        "# Read in the two files\n",
        "in1 = open(\"text1\").read()\n",
        "in2 = open(\"text2\").read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUYF0CGb2279",
        "outputId": "214c6a00-a968-4de1-b7f1-f33e84cd59bd"
      },
      "source": [
        "# Lowercasing the texts\n",
        "in1 = in1.lower()\n",
        "in2 = in2.lower()\n",
        "# Show preprocessing result\n",
        "pprint(in1)\n",
        "print(\"\\n\")\n",
        "pprint(in2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('in a prospective, double-blind, randomized trial the efficacy of a '\n",
            " 'heparinoid in ointment form was assessed in treating superficial '\n",
            " 'thrombophlebitis developing after continuous intravenous infusion. one '\n",
            " 'hundred surgical patients were studied, and clinical examination and the '\n",
            " 'iodine-125-labelled fibrinogen test used to assess the results. the mean '\n",
            " 'time required for the relief of local symptoms and signs and the rate of '\n",
            " 'local decline in radioactivity differed significantly between patients '\n",
            " 'receiving the heparinoid cream and those recieving the placebo.')\n",
            "\n",
            "\n",
            "('in a randomized, double-blind five-year trial, we tested the efficacy of '\n",
            " 'simultaneously elevating serum levels of high-density lipoprotein (hdl) '\n",
            " 'cholesterol and lowering levels of non-hdl cholesterol with gemfibrozil in '\n",
            " 'reducing the risk of coronary heart disease in 4081 asymptomatic middle-aged '\n",
            " 'men (40 to 55 years of age) with primary dyslipidemia (non-hdl cholesterol '\n",
            " 'greater than or equal to 200 mg per deciliter [5.2 mmol per liter] in two '\n",
            " 'consecutive pretreatment measurements). one group (2051 men) received 600 mg '\n",
            " 'of gemfibrozil twice daily, and the other (2030 men) received placebo. '\n",
            " 'gemfibrozil caused a marked increase in hdl cholesterol and persistent '\n",
            " 'reductions in serum levels of total, low-density lipoprotein (ldl), and '\n",
            " 'non-hdl cholesterol and triglycerides. there were minimal changes in serum '\n",
            " 'lipid levels in the placebo group. the cumulative rate of cardiac end points '\n",
            " 'at five years was 27.3 per 1,000 in the gemfibrozil group and 41.4 per 1,000 '\n",
            " 'in the placebo group--a reduction of 34.0 percent in the incidence of '\n",
            " 'coronary heart disease (95 percent confidence interval, 8.2 to 52.6; p less '\n",
            " 'than 0.02; two-tailed test). the decline in incidence in the gemfibrozil '\n",
            " 'group became evident in the second year and continued throughout the study. '\n",
            " 'there was no difference between the groups in the total death rate, nor did '\n",
            " 'the treatment influence the cancer rates. the results are in accord with two '\n",
            " 'previous trials with different pharmacologic agents and indicate that '\n",
            " 'modification of lipoprotein levels with gemfibrozil reduces the incidence of '\n",
            " 'coronary heart disease in men with dyslipidemia.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTS9TyV03hq_"
      },
      "source": [
        "b) Follow the instructions on https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "to vectorize the two texts using the CountVectorizer. Use n-gram size of 1-3 for the vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3c8ZG0V3tYw",
        "outputId": "e9a2afb8-c17d-49fa-9147-e17dacd10575"
      },
      "source": [
        "corpus = [in1, in2]\n",
        "# Create the count vectorizer instance\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "vec_corpus = vectorizer.fit_transform(corpus)\n",
        "# Get the first 10 feature names\n",
        "print(vectorizer.get_feature_names()[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['000', '000 in', '000 in the', '02', '02 two', '02 two tailed', '125', '125 labelled', '125 labelled fibrinogen', '200']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOXEBqOI6P5W",
        "outputId": "47672071-58f0-4eec-8b34-274f40553527"
      },
      "source": [
        "# Separate the vectors\n",
        "vect1 = vec_corpus[0]\n",
        "vect2 = vec_corpus[1]\n",
        "# Visualize the vectors\n",
        "print(\"Text1 Vector:\\n\", vect1.toarray(), \"\\n\")\n",
        "print(\"Text2 Vector:\\n\", vect2.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text1 Vector:\n",
            " [[0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 5 0 0 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0 1 1 2 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0\n",
            "  1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
            "  0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0\n",
            "  0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 0 0 0 1 1 1 4 0 0\n",
            "  0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0\n",
            "  0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 2 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0\n",
            "  0 0 2 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
            "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0\n",
            "  0 0 0 0 0 0 0 8 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1\n",
            "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0\n",
            "  0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0]] \n",
            "\n",
            "Text2 Vector:\n",
            " [[ 2  2  2  1  1  1  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "   0  0  0  1  1  1  1  1  1  1  1  1  8  1  1  0  0  1  1  1  1  1  1  1\n",
            "   1  1  1  0  0  1  0  1  0  0  0  1  1  1  1  1  0  0  0  0  0  0  1  1\n",
            "   1  1  1  1  1  1  1  1  0  0  1  1  1  1  1  0  0  1  1  1  1  1  1  1\n",
            "   1  1  1  1  1  5  3  1  1  1  1  1  1  1  0  0  0  1  1  1  1  1  1  1\n",
            "   1  1  0  0  0  3  3  3  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "   1  1  0  2  2  1  1  0  0  0  1  1  1  0  0  0  1  1  1  1  1  1  3  1\n",
            "   1  2  1  1  1  1  1  0  2  1  1  1  1  0  1  1  1  1  1  1  1  1  1  1\n",
            "   1  1  1  0  0  0  0  0  0  2  1  1  1  1  0  0  0  0  0  0  6  1  1  2\n",
            "   1  1  1  1  1  1  1  1  1  1  1  5  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "   1  5  5  3  1  1  3  3  1  2  0  0  0  0  0  1  1  1  0  0  0 17  1  1\n",
            "   1  1  1  1  1  1  1  1  0  0  0  0  0  0  1  1  1  1  2  1  1  7  2  1\n",
            "   2  1  1  0  0  1  1  3  1  1  2  2  1  1  1  1  1  1  1  1  1  0  0  0\n",
            "   1  1  1  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  5  1  1  3  1  1\n",
            "   1  1  1  1  1  1  3  1  1  1  1  1  1  1  1  1  0  0  0  0  0  1  1  1\n",
            "   1  1  1  1  1  1  0  0  0  1  1  1  4  1  1  2  1  1  1  1  2  1  1  1\n",
            "   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  3  3  3  1  1  1 12  1\n",
            "   1  1  1  1  1  3  3  1  1  0  0  1  1  1  1  0  0  0  1  1  1  1  1  1\n",
            "   0  0  0  1  1  1  0  0  1  1  1  1  1  1  0  0  0  0  0  4  2  2  1  1\n",
            "   1  1  2  1  1  1  1  1  1  1  1  1  1  3  1  1  2  1  1  1  1  1  1  1\n",
            "   1  1  1  1  1  1  1  0  0  0  0  0  0  1  1  1  0  0  2  1  1  1  1  0\n",
            "   1  1  1  2  1  1  1  1  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1\n",
            "   1  1  0  0  0  0  0  0  1  1  1  0  0  1  1  1  1  1  1  3  2  2  1  1\n",
            "   0  0  0  0  0  0  1  1  1  0  0  0  1  1  1  0  0  0  0  0  0  0  0  0\n",
            "   1  1  1  1  1  1  0  0  1  1  1  2  1  1  1  1  1  1  1 18  1  1  1  1\n",
            "   1  1  1  1  2  2  1  1  0  0  2  2  0  0  0  0  1  1  2  2  0  0  0  0\n",
            "   1  1  0  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  0  0  0  0  0  0\n",
            "   1  1  1  0  0  0  3  1  1  1  1  1  1  0  0  2  1  1  1  1  0  0  0  1\n",
            "   1  1  1  0  0  1  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  0\n",
            "   0  0  2  1  1  0  0  1  1  1  1  1  1  1  1  0  0  6  1  1  1  2  1  1\n",
            "   1  1  1  1  2  1  1  1  1  2  1  1  1  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LS_5P0u_9Jj"
      },
      "source": [
        "c) Compute the cosine similarity between the texts (sklearn provides a function for it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "MJRcSWdUISqY",
        "outputId": "62d56af5-b7d3-4930-9a8e-42f28e1dae16"
      },
      "source": [
        "# First convert the Sparse Matrix to dense and preview it\n",
        "dense_vec = vec_corpus.todense()\n",
        "df = pd.DataFrame(dense_vec, \n",
        "          columns=vectorizer.get_feature_names(),\n",
        "          index=[\"Text1\", \"Text2\"])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>000 in</th>\n",
              "      <th>000 in the</th>\n",
              "      <th>02</th>\n",
              "      <th>02 two</th>\n",
              "      <th>02 two tailed</th>\n",
              "      <th>125</th>\n",
              "      <th>125 labelled</th>\n",
              "      <th>125 labelled fibrinogen</th>\n",
              "      <th>200</th>\n",
              "      <th>200 mg</th>\n",
              "      <th>200 mg per</th>\n",
              "      <th>2030</th>\n",
              "      <th>2030 men</th>\n",
              "      <th>2030 men received</th>\n",
              "      <th>2051</th>\n",
              "      <th>2051 men</th>\n",
              "      <th>2051 men received</th>\n",
              "      <th>27</th>\n",
              "      <th>27 per</th>\n",
              "      <th>27 per 000</th>\n",
              "      <th>34</th>\n",
              "      <th>34 percent</th>\n",
              "      <th>34 percent in</th>\n",
              "      <th>40</th>\n",
              "      <th>40 to</th>\n",
              "      <th>40 to 55</th>\n",
              "      <th>4081</th>\n",
              "      <th>4081 asymptomatic</th>\n",
              "      <th>4081 asymptomatic middle</th>\n",
              "      <th>41</th>\n",
              "      <th>41 per</th>\n",
              "      <th>41 per 000</th>\n",
              "      <th>52</th>\n",
              "      <th>52 less</th>\n",
              "      <th>52 less than</th>\n",
              "      <th>55</th>\n",
              "      <th>55 years</th>\n",
              "      <th>55 years of</th>\n",
              "      <th>600</th>\n",
              "      <th>...</th>\n",
              "      <th>two tailed test</th>\n",
              "      <th>used</th>\n",
              "      <th>used to</th>\n",
              "      <th>used to assess</th>\n",
              "      <th>was</th>\n",
              "      <th>was 27</th>\n",
              "      <th>was 27 per</th>\n",
              "      <th>was assessed</th>\n",
              "      <th>was assessed in</th>\n",
              "      <th>was no</th>\n",
              "      <th>was no difference</th>\n",
              "      <th>we</th>\n",
              "      <th>we tested</th>\n",
              "      <th>we tested the</th>\n",
              "      <th>were</th>\n",
              "      <th>were minimal</th>\n",
              "      <th>were minimal changes</th>\n",
              "      <th>were studied</th>\n",
              "      <th>were studied and</th>\n",
              "      <th>with</th>\n",
              "      <th>with different</th>\n",
              "      <th>with different pharmacologic</th>\n",
              "      <th>with dyslipidemia</th>\n",
              "      <th>with gemfibrozil</th>\n",
              "      <th>with gemfibrozil in</th>\n",
              "      <th>with gemfibrozil reduces</th>\n",
              "      <th>with primary</th>\n",
              "      <th>with primary dyslipidemia</th>\n",
              "      <th>with two</th>\n",
              "      <th>with two previous</th>\n",
              "      <th>year</th>\n",
              "      <th>year and</th>\n",
              "      <th>year and continued</th>\n",
              "      <th>year trial</th>\n",
              "      <th>year trial we</th>\n",
              "      <th>years</th>\n",
              "      <th>years of</th>\n",
              "      <th>years of age</th>\n",
              "      <th>years was</th>\n",
              "      <th>years was 27</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Text1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 758 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       000  000 in  000 in the  ...  years of age  years was  years was 27\n",
              "Text1    0       0           0  ...             0          0             0\n",
              "Text2    2       2           2  ...             1          1             1\n",
              "\n",
              "[2 rows x 758 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9FFYcNfJeKn",
        "outputId": "ffddc48f-09df-41ba-99df-ecceac5b52a9"
      },
      "source": [
        "# Compute the cosine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cos_sim = cosine_similarity(df, df)[0, 1]\n",
        "print(\"The cosine similarity between the texts is: {:.2%}\".format(cos_sim))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between the texts is: 39.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1qV1np8UHa4",
        "outputId": "55c9a28d-f176-4cd8-fe00-50028225e117"
      },
      "source": [
        "!pip install shutup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shutup\n",
            "  Downloading shutup-0.1.3-py3-none-any.whl (1.3 kB)\n",
            "Installing collected packages: shutup\n",
            "Successfully installed shutup-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mpxj8MiKmqh"
      },
      "source": [
        "d) Compute the jaccard similarity between text1 and text2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF_15_BDKoZp",
        "outputId": "272c3d3a-7439-4ccc-e13a-2d52ace97f51"
      },
      "source": [
        "# Compute the jaccard similarity between text1 and text2\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "import shutup\n",
        "# Turn of the deprecation warning\n",
        "shutup.please()\n",
        "j_sim = jaccard_similarity_score(np.array(dense_vec[0]).flatten(), np.array(dense_vec[1]).flatten())\n",
        "print(\"The jaccard similarity between the texts is: {:.2%}\".format(j_sim))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The jaccard similarity between the texts is: 2.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myWWAy2KUzU1"
      },
      "source": [
        "2. Read section 4 and 5 of the nltk book: https://www.nltk.org/book/ch05.html.  \n",
        "a) Follow the instructions to train a POS tagger.  \n",
        "Show the performance of different taggers (e.g., unigram, bigram and combined taggers) on your chosen corpus (e.g. the brown corpus).  \n",
        "Use 90% of the data for training and 10% for evaluation.  \n",
        "Compare the performances of at least 3 taggers.  \n",
        "Save and load the tagger.  \n",
        "Use a trained tagger to tag all the words from text1.  \n",
        "Can lowercasing affect the performance of the POS tagger? (Put your answer as comment in python file)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF8g94KMV-GO"
      },
      "source": [
        "# Get the brown data set\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUpViXwxWRt1"
      },
      "source": [
        "# Train Test split\n",
        "train_tagged = brown.tagged_sents()[:round(0.9*len(brown.tagged_sents()))]\n",
        "test_tagged = brown.tagged_sents()[round(0.9*len(brown.tagged_sents())):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym7mvMyoXkeE",
        "outputId": "1ebeabd6-6800-4aa1-a382-dcbb923dc202"
      },
      "source": [
        "# Make sure that the split is correct\n",
        "len(train_tagged)/len(test_tagged) == 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD8Mq-FnYbiP",
        "outputId": "825a765a-bff0-46ee-b0ca-8132d299fd7e"
      },
      "source": [
        "import pickle\n",
        "# Compare the performance of difference taggers\n",
        "tagger_list = [nltk.DefaultTagger, nltk.UnigramTagger, nltk.BigramTagger]\n",
        "for tagger in tagger_list:\n",
        "  trained_tagger = tagger(train_tagged)\n",
        "  # evaluate the performance\n",
        "  print(tagger.__name__)\n",
        "  print(trained_tagger.evaluate(test_tagged), \"\\n\")\n",
        "\n",
        "# Combined tagger\n",
        "backoff_tagger = nltk.UnigramTagger(train_tagged)\n",
        "combined_tagger = nltk.BigramTagger(train_tagged, backoff=backoff_tagger)\n",
        "print(\"CombinedTagger\")\n",
        "print(combined_tagger.evaluate(test_tagged))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DefaultTagger\n",
            "0.0 \n",
            "\n",
            "UnigramTagger\n",
            "0.8849353534083527 \n",
            "\n",
            "BigramTagger\n",
            "0.3515747783994468 \n",
            "\n",
            "CombinedTagger\n",
            "0.9062362481926196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ4ukitPCp4t"
      },
      "source": [
        "As we can see that the CombinedTagger has the best performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diZreCBNc6jZ"
      },
      "source": [
        "# Save and load the taggers\n",
        "import pickle as pickle\n",
        "from sklearn.externals import joblib\n",
        "default_trained = nltk.DefaultTagger(train_tagged)\n",
        "Unigram_trained = nltk.UnigramTagger(train_tagged)\n",
        "Bigram_trained = nltk.BigramTagger(train_tagged)\n",
        "\n",
        "# Save and load the combined\n",
        "joblib.dump(combined_tagger, \"model.pkl\")\n",
        "combined_trained = joblib.load(\"model.pkl\")\n",
        "\n",
        "# Save and load the Unigram\n",
        "joblib.dump(Unigram_trained, \"model.pkl\")\n",
        "Unigram_trained = joblib.load(\"model.pkl\")\n",
        "\n",
        "# Save and load the bigram\n",
        "joblib.dump(Bigram_trained, \"model.pkl\")\n",
        "Bigram_trained = joblib.load(\"model.pkl\")"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFHkFuk4-s33",
        "outputId": "84b3cb37-e95a-4e57-80e3-047ffb0c4a89"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "# Tokenize the text\n",
        "in1_toke = word_tokenize(in1)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqZi1Hb49_RV",
        "outputId": "98d051ae-3b71-4533-fb99-953eab259866"
      },
      "source": [
        "# Show the first 20 tagged words\n",
        "combined_tagger.tag(in1_toke)[:20]"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('in', 'IN'),\n",
              " ('a', 'AT'),\n",
              " ('prospective', 'JJ'),\n",
              " (',', ','),\n",
              " ('double-blind', None),\n",
              " (',', ','),\n",
              " ('randomized', None),\n",
              " ('trial', 'NN'),\n",
              " ('the', 'AT'),\n",
              " ('efficacy', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('a', 'AT'),\n",
              " ('heparinoid', None),\n",
              " ('in', 'IN'),\n",
              " ('ointment', 'NN'),\n",
              " ('form', 'NN'),\n",
              " ('was', 'BEDZ'),\n",
              " ('assessed', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('treating', 'VBG')]"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puQjNh8E_0rB",
        "outputId": "c55c26a4-1ea1-4e2e-eb0b-83253c2a210b"
      },
      "source": [
        "# Is postag case sensitive?\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "in1_cap = open(\"text1\").read()\n",
        "nltk.pos_tag(in1_toke)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('in', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('prospective', 'JJ'),\n",
              " (',', ','),\n",
              " ('double-blind', 'JJ'),\n",
              " (',', ','),\n",
              " ('randomized', 'JJ'),\n",
              " ('trial', 'NN'),\n",
              " ('the', 'DT'),\n",
              " ('efficacy', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('heparinoid', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('ointment', 'JJ'),\n",
              " ('form', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('assessed', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('treating', 'VBG'),\n",
              " ('superficial', 'JJ'),\n",
              " ('thrombophlebitis', 'NN'),\n",
              " ('developing', 'VBG'),\n",
              " ('after', 'IN'),\n",
              " ('continuous', 'JJ'),\n",
              " ('intravenous', 'JJ'),\n",
              " ('infusion', 'NN'),\n",
              " ('.', '.'),\n",
              " ('one', 'CD'),\n",
              " ('hundred', 'VBD'),\n",
              " ('surgical', 'JJ'),\n",
              " ('patients', 'NNS'),\n",
              " ('were', 'VBD'),\n",
              " ('studied', 'VBN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('clinical', 'JJ'),\n",
              " ('examination', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('iodine-125-labelled', 'JJ'),\n",
              " ('fibrinogen', 'NN'),\n",
              " ('test', 'NN'),\n",
              " ('used', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('assess', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('results', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('the', 'DT'),\n",
              " ('mean', 'JJ'),\n",
              " ('time', 'NN'),\n",
              " ('required', 'VBN'),\n",
              " ('for', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('relief', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('local', 'JJ'),\n",
              " ('symptoms', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('signs', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('rate', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('local', 'JJ'),\n",
              " ('decline', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('radioactivity', 'NN'),\n",
              " ('differed', 'VBN'),\n",
              " ('significantly', 'RB'),\n",
              " ('between', 'IN'),\n",
              " ('patients', 'NNS'),\n",
              " ('receiving', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('heparinoid', 'NN'),\n",
              " ('cream', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('those', 'DT'),\n",
              " ('recieving', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('placebo', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM456tNrAj57",
        "outputId": "5a9db366-4ccd-404f-f9d6-d4281f49bb13"
      },
      "source": [
        "nltk.pos_tag(word_tokenize(in1_cap))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('In', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('prospective', 'JJ'),\n",
              " (',', ','),\n",
              " ('double-blind', 'JJ'),\n",
              " (',', ','),\n",
              " ('randomized', 'JJ'),\n",
              " ('trial', 'NN'),\n",
              " ('the', 'DT'),\n",
              " ('efficacy', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('heparinoid', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('ointment', 'JJ'),\n",
              " ('form', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('assessed', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('treating', 'VBG'),\n",
              " ('superficial', 'JJ'),\n",
              " ('thrombophlebitis', 'NN'),\n",
              " ('developing', 'VBG'),\n",
              " ('after', 'IN'),\n",
              " ('continuous', 'JJ'),\n",
              " ('intravenous', 'JJ'),\n",
              " ('infusion', 'NN'),\n",
              " ('.', '.'),\n",
              " ('One', 'CD'),\n",
              " ('hundred', 'VBD'),\n",
              " ('surgical', 'JJ'),\n",
              " ('patients', 'NNS'),\n",
              " ('were', 'VBD'),\n",
              " ('studied', 'VBN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('clinical', 'JJ'),\n",
              " ('examination', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('iodine-125-labelled', 'JJ'),\n",
              " ('fibrinogen', 'NN'),\n",
              " ('test', 'NN'),\n",
              " ('used', 'VBN'),\n",
              " ('to', 'TO'),\n",
              " ('assess', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('results', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('The', 'DT'),\n",
              " ('mean', 'JJ'),\n",
              " ('time', 'NN'),\n",
              " ('required', 'VBN'),\n",
              " ('for', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('relief', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('local', 'JJ'),\n",
              " ('symptoms', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('signs', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('rate', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('local', 'JJ'),\n",
              " ('decline', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('radioactivity', 'NN'),\n",
              " ('differed', 'VBN'),\n",
              " ('significantly', 'RB'),\n",
              " ('between', 'IN'),\n",
              " ('patients', 'NNS'),\n",
              " ('receiving', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('heparinoid', 'NN'),\n",
              " ('cream', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('those', 'DT'),\n",
              " ('recieving', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('placebo', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AurXdqzjCxFn"
      },
      "source": [
        "As we can see that the postag is case sensitive which means that lowercasing affect the performance of POS tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3shVU4ZDW-L"
      },
      "source": [
        "3. What are some of the possible NLP components of a Question Answering System?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_IzYwI_DX8m"
      },
      "source": [
        "Some of the possible components are:  \n",
        "**question classification, information retrieval, and answer extraction**  \n",
        "Question classification: The task of question classification is to predict theentity type of the answer of a natural language question. Question classification is typically done usingmachine learning techniques. Different lexical, syntactical and semantic features can be extracted from a question.  \n",
        "  \n",
        "Information Retrieval: Indexing the collection of documents: in this phase, NLP techniques are applied to generate an index containing document descriptions. Normally each document is described through a set of terms that, in theory, best represents its content.  \n",
        "  \n",
        "Answer Extraction: Aims to extract an exact answer from a relevant text snippet or a document. The motivation behind QA research is the need of user who is using state-of-the-art search engines.\n"
      ]
    }
  ]
}